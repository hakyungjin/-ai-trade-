"""
ìº”ë“¤ ë°ì´í„° ë°°ì¹˜ ì €ì¥ ìµœì í™”
- ëŒ€ëŸ‰ ë°ì´í„° íš¨ìœ¨ì  ì €ì¥
- ì¤‘ë³µ ì œê±°
- ë©”ëª¨ë¦¬ ìµœì í™”
"""

from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import asyncio
import logging
from sqlalchemy import insert, select, and_, func
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.market_data import MarketCandle

logger = logging.getLogger(__name__)


class CandleBatchSaver:
    """ëŒ€ëŸ‰ ìº”ë“¤ ë°ì´í„° íš¨ìœ¨ì  ì €ì¥"""
    
    BATCH_SIZE = 1000  # í•œ ë²ˆì— ì €ì¥í•  ìº”ë“¤ ê°œìˆ˜
    
    @staticmethod
    async def save_batch(
        db_session: AsyncSession,
        symbol: str,
        timeframe: str,
        candles: List[Dict[str, Any]],
        skip_duplicates: bool = True
    ) -> Dict[str, int]:
        """
        ìº”ë“¤ ë°ì´í„° ë°°ì¹˜ ì €ì¥ (ë©”ëª¨ë¦¬/ì‹œê°„ ìµœì í™”)
        
        Args:
            db_session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            symbol: ì‹¬ë³¼ (BTCUSDT ë“±)
            timeframe: íƒ€ì„í”„ë ˆì„ (1h, 4h ë“±)
            candles: ìº”ë“¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ [{'open': ..., 'high': ..., ...}]
            skip_duplicates: ì¤‘ë³µ ë¬´ì‹œ ì—¬ë¶€
        
        Returns:
            {'inserted': ê°œìˆ˜, 'skipped': ê°œìˆ˜, 'total': ê°œìˆ˜}
        """
        if not candles:
            return {'inserted': 0, 'skipped': 0, 'total': 0}
        
        stats = {'inserted': 0, 'skipped': 0, 'total': len(candles)}
        
        try:
            # ===== 1ï¸âƒ£ ê¸°ì¡´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¡°íšŒ (ì¤‘ë³µ í™•ì¸ìš©) =====
            if skip_duplicates:
                existing_timestamps = await CandleBatchSaver._get_existing_timestamps(
                    db_session, symbol, timeframe, candles
                )
            else:
                existing_timestamps = set()
            
            # ===== 2ï¸âƒ£ ë°°ì¹˜ ì¤€ë¹„ =====
            batch_data = []
            for candle in candles:
                # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
                open_time = CandleBatchSaver._parse_timestamp(candle.get('open_time'))
                
                # ì¤‘ë³µ í™•ì¸
                if skip_duplicates and open_time in existing_timestamps:
                    stats['skipped'] += 1
                    continue
                
                # ìº”ë“¤ ë°ì´í„° ì •ê·œí™”
                candle_row = {
                    'symbol': symbol,
                    'timeframe': timeframe,
                    'open_time': open_time,
                    'open': float(candle.get('open', 0)),
                    'high': float(candle.get('high', 0)),
                    'low': float(candle.get('low', 0)),
                    'close': float(candle.get('close', 0)),
                    'volume': float(candle.get('volume', 0)),
                    'close_time': CandleBatchSaver._parse_timestamp(candle.get('close_time')),
                    'quote_volume': float(candle.get('quote_volume', 0)),
                    'trades_count': int(candle.get('trades_count', 0)),
                }
                batch_data.append(candle_row)
            
            if not batch_data:
                logger.info(f"â­ï¸  No new candles for {symbol} {timeframe} (all duplicates)")
                return stats
            
            # ===== 3ï¸âƒ£ ë°°ì¹˜ ì €ì¥ (ì²­í¬ ë‹¨ìœ„) =====
            for i in range(0, len(batch_data), CandleBatchSaver.BATCH_SIZE):
                chunk = batch_data[i:i + CandleBatchSaver.BATCH_SIZE]
                
                stmt = insert(MarketCandle).values(chunk)
                # MySQL: IGNORE ì¤‘ë³µ, PostgreSQL: ON CONFLICT ë¬´ì‹œ
                await db_session.execute(stmt)
                
                stats['inserted'] += len(chunk)
                logger.debug(f"ğŸ“¤ Inserted {len(chunk)} candles ({i}/{len(batch_data)})")
            
            # ì»¤ë°‹
            await db_session.commit()
            logger.info(f"âœ… Saved {stats['inserted']} candles for {symbol} {timeframe}")
            
        except Exception as e:
            await db_session.rollback()
            logger.error(f"âŒ Error saving candles: {e}")
            stats['inserted'] = 0
            raise
        
        return stats
    
    @staticmethod
    async def save_multi_symbol(
        db_session: AsyncSession,
        data: Dict[str, Dict[str, List[Dict]]],  # {symbol: {timeframe: [candles]}}
        skip_duplicates: bool = True
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ì‹¬ë³¼/íƒ€ì„í”„ë ˆì„ì˜ ìº”ë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥
        
        Args:
            data: {
                'BTCUSDT': {'1h': [candles], '4h': [candles]},
                'ETHUSDT': {'1h': [candles]},
                ...
            }
        
        Returns:
            {
                'BTCUSDT': {'1h': {...}, '4h': {...}},
                'summary': {'total_inserted': ..., 'total_skipped': ...}
            }
        """
        results = {'summary': {'total_inserted': 0, 'total_skipped': 0}}
        
        for symbol, timeframes in data.items():
            results[symbol] = {}
            
            for timeframe, candles in timeframes.items():
                try:
                    stats = await CandleBatchSaver.save_batch(
                        db_session, symbol, timeframe, candles, skip_duplicates
                    )
                    results[symbol][timeframe] = stats
                    results['summary']['total_inserted'] += stats['inserted']
                    results['summary']['total_skipped'] += stats['skipped']
                    
                    # ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… (API ì œí•œ íšŒí”¼)
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    logger.error(f"âŒ Error saving {symbol} {timeframe}: {e}")
                    results[symbol][timeframe] = {
                        'inserted': 0,
                        'skipped': 0,
                        'error': str(e)
                    }
        
        return results
    
    @staticmethod
    async def _get_existing_timestamps(
        db_session: AsyncSession,
        symbol: str,
        timeframe: str,
        candles: List[Dict]
    ) -> set:
        """ê¸°ì¡´ íƒ€ì„ìŠ¤íƒí”„ ì¡°íšŒ (ì¤‘ë³µ í™•ì¸ìš©)"""
        # ì¡°íšŒí•  íƒ€ì„ìŠ¤íƒ¬í”„ ë²”ìœ„
        if not candles:
            return set()
        
        min_time = CandleBatchSaver._parse_timestamp(candles[0].get('open_time'))
        max_time = CandleBatchSaver._parse_timestamp(candles[-1].get('open_time'))
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ
        stmt = select(MarketCandle.open_time).where(
            and_(
                MarketCandle.symbol == symbol,
                MarketCandle.timeframe == timeframe,
                MarketCandle.open_time >= min_time,
                MarketCandle.open_time <= max_time
            )
        )
        
        result = await db_session.execute(stmt)
        existing = {row[0] for row in result.fetchall()}
        
        logger.debug(f"Found {len(existing)} existing candles for {symbol} {timeframe}")
        return existing
    
    @staticmethod
    def _parse_timestamp(ts) -> datetime:
        """íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± (ë°€ë¦¬ì´ˆ, ì´ˆ, datetime ì§€ì›)"""
        if isinstance(ts, datetime):
            return ts
        if isinstance(ts, (int, float)):
            # ë°€ë¦¬ì´ˆ ë‹¨ìœ„
            if ts > 1000000000000:
                return datetime.fromtimestamp(ts / 1000)
            # ì´ˆ ë‹¨ìœ„
            return datetime.fromtimestamp(ts)
        if isinstance(ts, str):
            try:
                return datetime.fromisoformat(ts)
            except:
                return datetime.fromtimestamp(int(ts) / 1000)
        return datetime.now()
    
    @staticmethod
    async def get_candle_stats(db_session: AsyncSession) -> Dict[str, Any]:
        """ìº”ë“¤ ì €ì¥ í˜„í™© ì¡°íšŒ"""
        try:
            # ì‹¬ë³¼ë³„ ìº”ë“¤ ê°œìˆ˜
            stmt = select(
                MarketCandle.symbol,
                MarketCandle.timeframe,
                func.count(MarketCandle.id).label('count'),
                func.min(MarketCandle.open_time).label('earliest'),
                func.max(MarketCandle.open_time).label('latest')
            ).group_by(MarketCandle.symbol, MarketCandle.timeframe)
            
            result = await db_session.execute(stmt)
            rows = result.fetchall()
            
            stats = {
                'by_symbol': {},
                'total_candles': 0,
                'total_symbols': 0,
                'total_timeframes': 0
            }
            
            for row in rows:
                symbol, timeframe, count, earliest, latest = row
                
                if symbol not in stats['by_symbol']:
                    stats['by_symbol'][symbol] = {}
                    stats['total_symbols'] += 1
                
                stats['by_symbol'][symbol][timeframe] = {
                    'count': count,
                    'earliest': earliest.isoformat() if earliest else None,
                    'latest': latest.isoformat() if latest else None,
                    'days_span': (latest - earliest).days if latest and earliest else 0
                }
                
                stats['total_candles'] += count
                stats['total_timeframes'] += 1
            
            return stats
        
        except Exception as e:
            logger.error(f"Error getting candle stats: {e}")
            return {'error': str(e)}
    
    @staticmethod
    async def cleanup_duplicates(db_session: AsyncSession) -> int:
        """ì¤‘ë³µ ìº”ë“¤ ë°ì´í„° ì œê±°"""
        try:
            # ì¤‘ë³µ ì œê±° ì¿¼ë¦¬
            # ê°™ì€ symbol/timeframe/open_timeì˜ ìµœì‹  idë§Œ ìœ ì§€
            logger.info("ğŸ§¹ Removing duplicate candles...")
            
            # MySQLì˜ ê²½ìš°
            stmt = """
            DELETE FROM market_candles 
            WHERE id NOT IN (
                SELECT MIN(id) FROM market_candles 
                GROUP BY symbol, timeframe, open_time
            );
            """
            
            result = await db_session.execute(stmt)
            deleted_count = result.rowcount
            
            await db_session.commit()
            logger.info(f"âœ… Deleted {deleted_count} duplicate candles")
            
            return deleted_count
        
        except Exception as e:
            logger.error(f"âŒ Error cleaning duplicates: {e}")
            await db_session.rollback()
            return 0
