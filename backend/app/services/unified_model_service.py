"""
Unified Model Service
í†µí•© LSTM ëª¨ë¸ì„ ì‚¬ìš©í•œ ì˜ˆì¸¡ ì„œë¹„ìŠ¤

Features:
- PyTorch í†µí•© ëª¨ë¸ ë¡œë“œ
- Asset ID ê¸°ë°˜ ì˜ˆì¸¡
- ì‹¤ì‹œê°„ ì¶”ë¡ 
"""

import os
import sys
import logging
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from typing import Dict, Any, List, Optional
from pathlib import Path
import joblib

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
PROJECT_ROOT = Path(__file__).parent.parent.parent
AI_MODEL_PATH = PROJECT_ROOT / "ai-model"
sys.path.insert(0, str(AI_MODEL_PATH))

from app.services.technical_indicators import TechnicalIndicators
from app.services.unified_feature_engineering import compute_all_features
from app.services.asset_mapping_service import AssetMappingService
from models.unified_lstm_model import UnifiedLSTMModel

logger = logging.getLogger(__name__)


class UnifiedModelService:
    """í†µí•© ëª¨ë¸ ì˜ˆì¸¡ ì„œë¹„ìŠ¤"""

    # ë ˆì´ë¸” ë§¤í•‘
    LABEL_MAP_3 = {
        0: ('SELL', -1),
        1: ('HOLD', 0),
        2: ('BUY', 1)
    }

    LABEL_MAP_5 = {
        0: ('STRONG_SELL', -2),
        1: ('SELL', -1),
        2: ('HOLD', 0),
        3: ('BUY', 1),
        4: ('STRONG_BUY', 2)
    }

    def __init__(
        self,
        model_path: Optional[str] = None,
        scaler_path: Optional[str] = None,
        features_path: Optional[str] = None,
        device: str = None
    ):
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.is_loaded = False
        self.num_classes = 3
        self.sequence_length = 60
        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')

        # ë ˆì´ë¸” ë§¤í•‘
        self.label_map = self.LABEL_MAP_3

        if model_path and scaler_path and features_path:
            self.load_model(model_path, scaler_path, features_path)

    def load_model(self, model_path: str, scaler_path: str, features_path: str) -> bool:
        """í†µí•© ëª¨ë¸ ë¡œë“œ"""
        try:
            if not os.path.exists(model_path):
                logger.warning(f"Model file not found: {model_path}")
                return False

            # Checkpoint ë¡œë“œ
            checkpoint = torch.load(model_path, map_location=self.device)

            # ëª¨ë¸ êµ¬ì¡° ì¬êµ¬ì„± (checkpointì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ)
            # ì‹¤ì œë¡œëŠ” ëª¨ë¸ configë„ í•¨ê»˜ ì €ì¥í•´ì•¼ í•¨
            self.model = UnifiedLSTMModel(
                num_assets=500,  # ì„¤ì •ì—ì„œ ì½ì–´ì•¼ í•¨
                embedding_dim=16,
                num_features=100,  # features_pathì—ì„œ ê³„ì‚°
                hidden_size=64,
                num_classes=3,
                num_layers=2,
                dropout=0.3
            )

            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()

            # Scaler & Features ë¡œë“œ
            self.scaler = joblib.load(scaler_path)
            self.feature_names = joblib.load(features_path)

            self.num_classes = checkpoint.get('num_classes', 3)
            self.label_map = self.LABEL_MAP_3 if self.num_classes == 3 else self.LABEL_MAP_5

            self.is_loaded = True

            logger.info(f"âœ… Loaded unified model from {model_path}")
            logger.info(f"   Device: {self.device}")
            logger.info(f"   Features: {len(self.feature_names)}, Classes: {self.num_classes}")
            logger.info(f"   Sequence length: {self.sequence_length}")

            return True

        except Exception as e:
            logger.error(f"âŒ Failed to load unified model: {e}")
            import traceback
            logger.error(traceback.format_exc())
            self.is_loaded = False
            return False

    async def predict(
        self,
        symbol: str,
        candles: List[Dict[str, Any]],
        db
    ) -> Dict[str, Any]:
        """
        í†µí•© ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰

        Args:
            symbol: ìì‚° ì‹¬ë³¼ (BTCUSDT, AAPL ë“±)
            candles: ìº”ë“¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ìµœì†Œ 60ê°œ)
            db: DB ì„¸ì…˜ (Asset ID ì¡°íšŒìš©)

        Returns:
            {
                'signal': 'BUY' | 'SELL' | 'HOLD',
                'detailed_signal': 'STRONG_BUY' | 'BUY' | 'HOLD' | 'SELL' | 'STRONG_SELL',
                'confidence': 0.0 ~ 1.0,
                'probabilities': {...},
                'asset_id': int
            }
        """
        if not self.is_loaded:
            return self._default_response("Unified model not loaded")

        if len(candles) < self.sequence_length:
            return self._default_response(
                f"Insufficient data: {len(candles)} candles (min {self.sequence_length})"
            )

        try:
            # 1. Asset ID ì¡°íšŒ
            asset_id = await AssetMappingService.get_asset_id(db, symbol, create_if_missing=True)
            if asset_id is None:
                return self._default_response(f"Failed to get asset_id for {symbol}")

            logger.info(f"ğŸ” Predicting for {symbol} (asset_id={asset_id})...")

            # 2. DataFrame ìƒì„± ë° í”¼ì²˜ ê³„ì‚°
            df = pd.DataFrame(candles)

            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce')

            # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
            df = TechnicalIndicators.calculate_all_indicators(df)

            # í†µí•© í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
            df = compute_all_features(df)

            # NaN ì²˜ë¦¬
            df[self.feature_names] = df[self.feature_names].fillna(0)
            df[self.feature_names] = df[self.feature_names].replace([np.inf, -np.inf], 0)

            # 3. ë§ˆì§€ë§‰ 60ê°œ ìº”ë“¤ ì¶”ì¶œ
            if len(df) < self.sequence_length:
                return self._default_response(
                    f"Not enough data after processing: {len(df)} < {self.sequence_length}"
                )

            last_60 = df[self.feature_names].iloc[-self.sequence_length:].values

            # 4. ì •ê·œí™”
            last_60_scaled = self.scaler.transform(last_60)

            # 5. Tensor ë³€í™˜
            time_series = torch.FloatTensor(last_60_scaled).unsqueeze(0).to(self.device)  # (1, 60, features)
            asset_id_tensor = torch.LongTensor([asset_id]).to(self.device)  # (1,)

            # 6. ì˜ˆì¸¡
            with torch.no_grad():
                logits = self.model(time_series, asset_id_tensor)
                probabilities = F.softmax(logits, dim=1)
                predicted_class = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][predicted_class].item()

            # 7. ê²°ê³¼ ë§¤í•‘
            detailed_signal, signal_value = self.label_map[predicted_class]
            simple_signal = self._to_simple_signal(detailed_signal)

            # í™•ë¥  ë”•ì…”ë„ˆë¦¬
            prob_dict = {}
            for class_id, (label, _) in self.label_map.items():
                if class_id < probabilities.size(1):
                    prob_dict[label] = float(probabilities[0][class_id].item())

            result = {
                'signal': simple_signal,
                'detailed_signal': detailed_signal,
                'signal_value': signal_value,
                'confidence': confidence,
                'direction': 'UP' if signal_value > 0 else ('DOWN' if signal_value < 0 else 'NEUTRAL'),
                'probabilities': prob_dict,
                'asset_id': asset_id,
                'analysis': self._generate_analysis(detailed_signal, confidence, prob_dict)
            }

            logger.info(f"âœ… Prediction: {simple_signal} ({detailed_signal}) with {confidence:.2%} confidence")

            return result

        except Exception as e:
            logger.error(f"âŒ Prediction error: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self._default_response(f"Prediction error: {str(e)}")

    def _to_simple_signal(self, detailed_signal: str) -> str:
        """ìƒì„¸ ì‹ í˜¸ â†’ ê°„ë‹¨í•œ ì‹ í˜¸ ë³€í™˜"""
        mapping = {
            'STRONG_SELL': 'SELL',
            'SELL': 'SELL',
            'HOLD': 'HOLD',
            'BUY': 'BUY',
            'STRONG_BUY': 'BUY'
        }
        return mapping.get(detailed_signal, 'HOLD')

    def _generate_analysis(
        self,
        detailed_signal: str,
        confidence: float,
        probabilities: Dict[str, float]
    ) -> str:
        """ë¶„ì„ í…ìŠ¤íŠ¸ ìƒì„±"""
        confidence_text = "ë†’ì€" if confidence > 0.7 else ("ì¤‘ê°„" if confidence > 0.5 else "ë‚®ì€")

        analysis = f"{detailed_signal} ì‹ í˜¸ê°€ {confidence_text} í™•ì‹ ë„({confidence:.1%})ë¡œ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
        analysis += "í™•ë¥  ë¶„í¬:\n"

        for label, prob in sorted(probabilities.items(), key=lambda x: x[1], reverse=True):
            bar = "â–ˆ" * int(prob * 20)
            analysis += f"  {label:15s}: {prob:5.1%} {bar}\n"

        return analysis

    def _default_response(self, message: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ì‘ë‹µ (ì—ëŸ¬ ì‹œ)"""
        logger.warning(f"âš ï¸ Default response: {message}")
        return {
            'signal': 'HOLD',
            'detailed_signal': 'HOLD',
            'signal_value': 0,
            'confidence': 0.0,
            'direction': 'NEUTRAL',
            'probabilities': {'HOLD': 1.0},
            'analysis': f"ì˜ˆì¸¡ ì‹¤íŒ¨: {message}",
            'error': message
        }


# ===== ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤) =====
_unified_service_instance: Optional[UnifiedModelService] = None


def get_unified_service(force_reload: bool = False) -> Optional[UnifiedModelService]:
    """í†µí•© ëª¨ë¸ ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _unified_service_instance

    if _unified_service_instance is None or force_reload:
        # ìµœì‹  ëª¨ë¸ íŒŒì¼ ì°¾ê¸°
        models_dir = AI_MODEL_PATH / "models"

        # unified_YYYYMMDD_HHMMSS ë””ë ‰í† ë¦¬ ì°¾ê¸°
        unified_dirs = sorted(models_dir.glob("unified_*"), reverse=True)

        if not unified_dirs:
            logger.warning("âš ï¸ No unified model directories found")
            return None

        model_dir = unified_dirs[0]
        model_path = model_dir / "unified_model_best.pt"
        scaler_path = model_dir / "unified_scaler.joblib"
        features_path = model_dir / "unified_features.joblib"

        if not model_path.exists():
            logger.warning(f"âš ï¸ Model file not found: {model_path}")
            return None

        logger.info(f"ğŸ“‚ Loading unified model from {model_dir}")

        _unified_service_instance = UnifiedModelService(
            model_path=str(model_path),
            scaler_path=str(scaler_path),
            features_path=str(features_path)
        )

    return _unified_service_instance if _unified_service_instance and _unified_service_instance.is_loaded else None
